<br />
<div align="center">
  <a href="https://github.com/MiliLab/PEARL">
    <img src="images/icon.png" alt="Logo" width="80" height="80">
  </a>

  <h3 align="center">PEARL</h3>

  <p align="center">
    Perceptual-Evidence Anchored Reinforced Learning for Multimodal Reasoning
    <br />
    <a href="https://github.com/MiliLab/PEARL">Paper</a>
    &middot;
    <a href="https://huggingface.co/Rex1090/PEARL-7B">Model</a>
    &middot;
    <a href="https://huggingface.co/Rex1090/PEARL-7B">Dataset (Coming Soon)</a>
  </p>
</div>

## About PEARL
PEARL (Perceptual-Evidence Anchored Reinforced Learning), a dual-branch, perception-reasoning synergistic that strengthens multimodal reasoning by explicitly anchoring it to verified visual evidence. 

<p align="center">
    <img src="images/struct.png" width="60%">
</p>

## ToDo

- [x] Inference Code of PEARL-7B (Based on Qwen2.5-VL-7B)
- [ ] Training Code of PEARL
- [ ] PEARL-3B (Based on Qwen2.5-VL-3B), PEARL-32B (Based on Qwen2.5-VL-32B)
- [ ] PEARL-8B (Based on Qwen3-VL-8B)
- [ ] Perception Probes ([ViRL39k](https://huggingface.co/datasets/TIGER-Lab/ViRL39K), [Geo3k](https://huggingface.co/datasets/hiyouga/geometry3k), [MMK12](https://huggingface.co/datasets/FanqingM/MMK12/viewer/default/train?p=2))

